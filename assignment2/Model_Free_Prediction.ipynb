{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a957b574-21d2-418f-84c7-22532d721458",
   "metadata": {},
   "source": [
    "# Inleveropgave 2: Model-Free Prediction and Control\n",
    "\n",
    "## Model-Free Prediction\n",
    "\n",
    "### Sources\n",
    "- https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3598de3e-82f0-4eaf-bf8b-91339b517f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from collections import defaultdict\n",
    "from utils import Maze, show_utility, value_iteration\n",
    "\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb13a94-7c2f-4e6e-a870-b7ba6f5017dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43907e9-1ddd-4a20-adab-ce08854c1e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.5   , 35.    , 40.    ,  0.    ],\n",
       "       [26.45  , 30.5   , 35.    , 40.    ],\n",
       "       [22.805 , 26.45  , 22.805 , 26.    ],\n",
       "       [ 0.    , 22.805 , 19.5245, 22.4   ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_state = (3, 2)\n",
    "terminal_states = [(0, 3), (3, 0)]\n",
    "\n",
    "rewards = np.array([[-1, -1, -1, 40],\n",
    "                    [-1, -1, -10, -10],\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [10, -2, -1, -1]])\n",
    "\n",
    "# initialize the Maze\n",
    "maze = Maze(rewards, terminal_states, start_state)\n",
    "\n",
    "# use the value function to get the utilities\n",
    "values = value_iteration(maze, discount=0.9, p_action=1.0)\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb4fd2-06be-4ad4-83f1-e1ef5a7d3009",
   "metadata": {},
   "source": [
    "### Generating episodes\n",
    "Voor het genereren van een episode worden er twee nieuwe functies ge√Øntroduceerd. De eerste functie kan gebruikt worden om een random episode te genereren, terwijl de tweede gebruikt maakt van de eerder uitgewerkte value iteration om zodanig een bepaalde policy te kunnen volgen.\n",
    "\n",
    "Deze functies hebben dezelfde hoeveelheid parameters. Echter, niet al deze parameters worden gebruikt (dit zorgt voor iets meer consistency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb4a389-b682-49e7-b53d-969057e25b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_random(env: Maze, values: np.ndarray, discount: float):\n",
    "    \"\"\"\"\"\"\n",
    "    steps = []  # holds Tuples with the states, actions and rewards\n",
    "    pos = env.get_random_position()\n",
    "    \n",
    "    # break if the chosen state is a terminal state\n",
    "    while pos not in env.end_states:\n",
    "\n",
    "        next_actions = env.get_next_action_positions(pos)\n",
    "        # choose a random action and gather\n",
    "        action = rd.choice(next_actions)\n",
    "        \n",
    "        reward = env.R[action]\n",
    "        steps.append((pos, action, reward))\n",
    "        # update the pos to the taken action\n",
    "        pos = action\n",
    "        \n",
    "    # save the latest pos with all extra data\n",
    "    steps.append((pos, (), 0))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5665c6-82de-4a1c-bcc0-4bcf20f5ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_optimal(env: Maze, values: np.ndarray, discount: float):\n",
    "    \"\"\"\"\"\"\n",
    "    steps = []  # holds Tuples with the states, actions and rewards\n",
    "    pos = env.get_random_position()\n",
    "    \n",
    "    # break if the chosen state is a terminal state\n",
    "    while pos not in env.end_states:\n",
    "    \n",
    "        # get the next action based on the optimal policy\n",
    "        next_actions = env.get_next_action_positions(pos)\n",
    "        action_values = []\n",
    "        \n",
    "        for action in next_actions:\n",
    "            action_values.append(env.R[action] + (discount * values[action]))\n",
    "        \n",
    "        # get the index of the max elements \n",
    "        max_elem = max(action_values)\n",
    "        policy_actions = [act for i, act in zip(action_values, next_actions) if i == max_elem]\n",
    "        \n",
    "        # choose a random action and gather\n",
    "        action = rd.choice(policy_actions)\n",
    "        \n",
    "        reward = env.R[action]\n",
    "        steps.append((pos, action, reward))\n",
    "        # update the pos to the taken action\n",
    "        pos = action\n",
    "        \n",
    "    # save the latest pos with all extra data\n",
    "    steps.append((pos, (), 0))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db010d7-3439-434b-9ef9-fceaa9932385",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1249c51-3b13-4b32-89b2-62ebcecf3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_evaluation(env: Maze, values: np.ndarray, policy: callable, discount: float = 0.9,\n",
    "                              n_episodes: int = 1000, p_action: float = 0.7):\n",
    "    \"\"\"\"\"\"\n",
    "    state_values = np.zeros(env.R.shape)\n",
    "    state_returns = defaultdict(list)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # generate a new episode with a certain policy\n",
    "        episode = policy(env, values, discount)\n",
    "\n",
    "        G = 0\n",
    "        visited_states = []\n",
    "        # looping over each step and \n",
    "        for pos, action, reward in episode[::-1]:\n",
    "            G = discount * G + reward\n",
    "            \n",
    "            if pos not in visited_states:\n",
    "                # update the the current state with the new return\n",
    "                state_returns[pos].append(G)\n",
    "                # calculate the average value\n",
    "                state_values[pos] = np.mean(state_returns[pos])\n",
    "                # update visited states\n",
    "                visited_states.append(pos)\n",
    "    \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f336f-4141-4463-8a1a-371efe97f776",
   "metadata": {},
   "source": [
    "#### MC Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32ccf42-5f32-45dc-94cf-a85b09b32ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -2.71  | 5.85   | 14.75  | 0.0    | \n",
      "-------------------------------------\n",
      "| 1.09   | 5.37   | 14.17  | 19.12  | \n",
      "-------------------------------------\n",
      "| 4.77   | 4.26   | 4.53   | -0.1   | \n",
      "-------------------------------------\n",
      "| 0.0    | 1.11   | -1.66  | -5.94  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_random, discount=1.0)\n",
    "show_utility(random_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1dcbbc-4e8b-4c92-9bdf-763081bd5a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -1.3   | 6.12   | 18.82  | 0.0    | \n",
      "-------------------------------------\n",
      "| -0.38  | 1.88   | 11.51  | 19.54  | \n",
      "-------------------------------------\n",
      "| 3.56   | 0.82   | 0.07   | -0.96  | \n",
      "-------------------------------------\n",
      "| 0.0    | 4.88   | -1.12  | -4.08  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values2 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_random, discount=0.9)\n",
    "show_utility(random_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8d5cf-27a5-4a1f-b97d-d1c9fe230379",
   "metadata": {},
   "source": [
    "#### MC Optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f5d4ea-4d26-44ee-955f-7b50e0534ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 38.0   | 39.0   | 40.0   | 0.0    | \n",
      "-------------------------------------\n",
      "| 37.0   | 38.0   | 39.0   | 40.0   | \n",
      "-------------------------------------\n",
      "| 36.0   | 37.0   | 36.0   | 30.0   | \n",
      "-------------------------------------\n",
      "| 0.0    | 36.0   | 35.0   | 29.0   | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_optimal, discount=1.0, n_episodes=100)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab0bcd9-704d-4885-a0fa-e17cc7197f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 30.5   | 35.0   | 40.0   | 0.0    | \n",
      "-------------------------------------\n",
      "| 26.45  | 30.5   | 35.0   | 40.0   | \n",
      "-------------------------------------\n",
      "| 22.81  | 26.45  | 22.8   | 26.0   | \n",
      "-------------------------------------\n",
      "| 0.0    | 22.8   | 19.52  | 22.4   | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_optimal, discount=0.9, n_episodes=100)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24275c94-340b-4709-bbb4-b42c67d88d15",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6822f97-46ed-477d-a47a-7f7c3cdb42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference_learning(env: Maze, values: np.ndarray, policy: callable, step_size: float = 0.01,\n",
    "                                 discount: float = 0.9, n_episodes: int = 1000):\n",
    "    \"\"\"\"\"\"\n",
    "    state_values = np.zeros(env.R.shape)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # generate a new episode with a certain policy\n",
    "        episode = policy(env, values, discount)\n",
    "        \n",
    "        for pos, action, reward in episode:\n",
    "            if pos in env.end_states:\n",
    "                break\n",
    "            else:\n",
    "                value = state_values[pos]\n",
    "                state_values[pos] = value + step_size * (reward + discount * state_values[action] - value)\n",
    "    \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517497cd-54f9-4c55-b278-bbdf1d7beb68",
   "metadata": {},
   "source": [
    "#### TD Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c534a7-ae4a-4542-9022-c06b5e1699df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -7.34  | -4.2   | 6.41   | 0.0    | \n",
      "-------------------------------------\n",
      "| -7.34  | -9.59  | -5.68  | 5.77   | \n",
      "-------------------------------------\n",
      "| -1.28  | -7.34  | -11.12 | -9.89  | \n",
      "-------------------------------------\n",
      "| 0.0    | -2.56  | -9.34  | -11.45 | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values1 = temporal_difference_learning(maze, values, step_size=0.01, policy=generate_episode_random, discount=1.0)\n",
    "show_utility(random_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3f170209-dc80-4ae8-8a56-c572b0baba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -4.28  | -2.05  | 7.36   | 0.0    | \n",
      "-------------------------------------\n",
      "| -4.06  | -6.2   | -4.01  | 2.1    | \n",
      "-------------------------------------\n",
      "| 0.55   | -4.2   | -7.67  | -7.39  | \n",
      "-------------------------------------\n",
      "| 0.0    | -0.53  | -5.53  | -6.88  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values2 = temporal_difference_learning(maze, values, step_size=0.01, policy=generate_episode_random, discount=0.9)\n",
    "show_utility(random_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fe73a-dae8-4366-a265-89216b3a4325",
   "metadata": {},
   "source": [
    "#### TD Optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "741e77aa-2461-4145-b5c8-6e291b61545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 21.12  | 38.46  | 39.96  | 0.0    | \n",
      "-------------------------------------\n",
      "| 9.5    | 33.63  | 14.72  | 33.58  | \n",
      "-------------------------------------\n",
      "| 1.67   | 21.93  | 6.61   | 10.72  | \n",
      "-------------------------------------\n",
      "| 0.0    | 3.5    | 0.38   | 1.38   | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = temporal_difference_learning(maze, values, step_size=0.01, policy=generate_episode_optimal, discount=1.0)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "788e0adf-9a98-4a46-a86d-c35de1600159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 13.99  | 34.37  | 39.95  | 0.0    | \n",
      "-------------------------------------\n",
      "| 3.83   | 26.41  | 16.3   | 34.01  | \n",
      "-------------------------------------\n",
      "| 1.02   | 15.27  | 4.13   | 9.27   | \n",
      "-------------------------------------\n",
      "| 0.0    | 1.55   | -0.01  | 0.72   | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values2 = temporal_difference_learning(maze, values, step_size=0.01, policy=generate_episode_optimal, discount=0.9)\n",
    "show_utility(optim_values2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
