{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a957b574-21d2-418f-84c7-22532d721458",
   "metadata": {},
   "source": [
    "# Inleveropgave 2: Model-Free Prediction and Control\n",
    "\n",
    "## Model-Free Prediction\n",
    "\n",
    "### Sources\n",
    "- https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3598de3e-82f0-4eaf-bf8b-91339b517f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from collections import defaultdict\n",
    "from utils import Maze, show_utility, value_iteration\n",
    "\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb13a94-7c2f-4e6e-a870-b7ba6f5017dc",
   "metadata": {},
   "source": [
    "Eerst wordt de environment, terminal states en de values van de value iteration geïnitialiseerd."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43907e9-1ddd-4a20-adab-ce08854c1e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.5   , 35.    , 40.    ,  0.    ],\n",
       "       [26.45  , 30.5   , 35.    , 40.    ],\n",
       "       [22.805 , 26.45  , 22.805 , 26.    ],\n",
       "       [ 0.    , 22.805 , 19.5245, 22.4   ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_state = (3, 2)\n",
    "terminal_states = [(0, 3), (3, 0)]\n",
    "\n",
    "rewards = np.array([[-1, -1, -1, 40],\n",
    "                    [-1, -1, -10, -10],\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [10, -2, -1, -1]])\n",
    "\n",
    "# initialize the Maze\n",
    "maze = Maze(rewards, terminal_states, start_state)\n",
    "\n",
    "# use the value function to get the utilities\n",
    "values = value_iteration(maze, discount=0.9, p_action=1.0)\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb4fd2-06be-4ad4-83f1-e1ef5a7d3009",
   "metadata": {},
   "source": [
    "### Generating episodes\n",
    "Voor het genereren van een episode worden er twee nieuwe functies geïntroduceerd. De eerste functie kan gebruikt worden om een random episode te genereren, terwijl de tweede gebruikt maakt van de eerder uitgewerkte value iteration om zodanig een bepaalde policy te kunnen volgen.\n",
    "\n",
    "Deze functies hebben dezelfde hoeveelheid parameters. Echter, niet al deze parameters worden gebruikt (dit zorgt voor iets meer consistency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb4a389-b682-49e7-b53d-969057e25b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_random(env: Maze, values: np.ndarray, discount: float, p_action: float) -> List[Tuple[Tuple[int, int], Tuple[int, int], int]]:\n",
    "    \"\"\"Generates an episode based on the random policy.\n",
    "    \n",
    "    Here the p_action is not being used, because the actions are already random and therefore it won't really matter if we take a wrong turn\n",
    "    Furthermore the discount is not being used.\n",
    "    Both these parameters are set in the function because API consistency\n",
    "    \n",
    "    args:\n",
    "        env (Maze): The environment which holds the rewards of all the possible states.\n",
    "        values (np.ndarray): The utility values of the states.\n",
    "        discount (float): The discount factor.\n",
    "        p_action (float): The probability of a succesfull action.        \n",
    "\n",
    "    returns:\n",
    "        List[Tuple[Tuple[int, int], Tuple[int, int], int]]: Returns a list with all the state-action pairs with the corresponding rewards.\n",
    "    \"\"\"\n",
    "    steps = []  # holds Tuples with the states, actions and rewards\n",
    "    pos = env.get_random_position()\n",
    "    \n",
    "    # break if the chosen state is a terminal state\n",
    "    while pos not in env.end_states:\n",
    "\n",
    "        next_actions = env.get_next_action_positions(pos)\n",
    "        # choose a random action and get the reward for the action.\n",
    "        action = rd.choice(next_actions)\n",
    "        \n",
    "        reward = env.R[action]\n",
    "        steps.append((pos, action, reward))\n",
    "        # update the pos to the taken action\n",
    "        pos = action\n",
    "        \n",
    "    # save the latest pos with all extra data\n",
    "    steps.append((pos, (), 0))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5665c6-82de-4a1c-bcc0-4bcf20f5ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_optimal(env: Maze, values: np.ndarray, discount: float, p_action: float) -> List[Tuple[Tuple[int, int], Tuple[int, int], int]]:\n",
    "    \"\"\"Generates an episode based on the optimal policy.\n",
    "\n",
    "    args:\n",
    "        env (Maze): The environment which holds the rewards of all the possible states.\n",
    "        values (np.ndarray): The utility values of the states.\n",
    "        discount (float): The discount factor.\n",
    "        p_action (float): The probability of a succesfull action.        \n",
    "\n",
    "    returns:\n",
    "        List[Tuple[Tuple[int, int], Tuple[int, int], int]]: Returns a list with all the state-action pairs with the corresponding rewards.\n",
    "    \"\"\"\n",
    "    steps = []  # holds Tuples with the states, actions and rewards\n",
    "    pos = env.get_random_position()\n",
    "    \n",
    "    # break if the chosen state is a terminal state\n",
    "    while pos not in env.end_states:\n",
    "    \n",
    "        # get the next action based on the optimal policy\n",
    "        next_actions = env.get_next_action_positions(pos)\n",
    "        action_values = []\n",
    "        \n",
    "        for action in next_actions:\n",
    "            action_values.append(env.R[action] + (discount * values[action]))\n",
    "        \n",
    "        # get the index of the max elements\n",
    "        max_elem = max(action_values)\n",
    "        policy_actions = [act for i, act in zip(action_values, next_actions) if i == max_elem]\n",
    "        \n",
    "        # choose the desired action and check based on the p_action if the action is certain\n",
    "        action = rd.choice(policy_actions)\n",
    "        if p_action < rd.random():\n",
    "            # whoops, the desired action cannot be taken, so choose one of the others\n",
    "            chosen_index = np.argmax(action_values)\n",
    "            # remove the earlier chosen action and choose a random action\n",
    "            action = rd.choice(next_actions[:chosen_index] + next_actions[chosen_index + 1:])\n",
    "        \n",
    "        reward = env.R[action]\n",
    "        steps.append((pos, action, reward))\n",
    "        # update the pos to the taken action\n",
    "        pos = action\n",
    "        \n",
    "    # save the latest pos with all extra data\n",
    "    steps.append((pos, (), 0))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db010d7-3439-434b-9ef9-fceaa9932385",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1249c51-3b13-4b32-89b2-62ebcecf3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_evaluation(env: Maze, values: np.ndarray, policy: callable, discount: float = 0.9,\n",
    "                              n_episodes: int = 10000, p_action: float = 0.7) -> np.ndarray:\n",
    "    \"\"\"A function that uses monte carlo policy evaluation to get a value-function.\n",
    "\n",
    "    args:\n",
    "        env (Maze): The environment which holds the rewards of all the possible states.\n",
    "        values (np.ndarray): The utility values of the states.\n",
    "        policy (callable): The policy that is used to generate an episode.\n",
    "        discount (float, optional): The discount factor. Defaults to 0.9.\n",
    "        n_episodes (int, optional): The amount of episodes to run. Defaults to 10000.\n",
    "        p_action (float, optional): The probability of a succesfull action. Defaults to 0.7.        \n",
    "\n",
    "    returns:\n",
    "        np.ndarray: Returns the calculated values per state.\n",
    "    \"\"\"\n",
    "    state_values = np.zeros(env.R.shape)\n",
    "    state_returns = defaultdict(list)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # generate a new episode with a certain policy\n",
    "        episode = policy(env, values, discount, p_action)\n",
    "\n",
    "        G = 0\n",
    "        visited_states = []\n",
    "        # looping over each step and \n",
    "        for pos, action, reward in episode[::-1]:\n",
    "            G = discount * G + reward\n",
    "            \n",
    "            if pos not in visited_states:\n",
    "                # update the the current state with the new return\n",
    "                state_returns[pos].append(G)\n",
    "                # calculate the average value\n",
    "                state_values[pos] = np.mean(state_returns[pos])\n",
    "                # update visited states\n",
    "                visited_states.append(pos)\n",
    "    \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f336f-4141-4463-8a1a-371efe97f776",
   "metadata": {},
   "source": [
    "#### MC Random policy\n",
    "met discount van 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32ccf42-5f32-45dc-94cf-a85b09b32ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -1.08  | 4.58   | 15.71  | 0.0    | \n",
      "-------------------------------------\n",
      "| 0.97   | 4.69   | 14.89  | 19.39  | \n",
      "-------------------------------------\n",
      "| 2.98   | 3.61   | 4.5    | 2.62   | \n",
      "-------------------------------------\n",
      "| 0.0    | 3.49   | -0.32  | -3.26  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_random, discount=1.0)\n",
    "show_utility(random_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7de0c295-35a4-4a76-8219-388129eb7d34",
   "metadata": {},
   "source": [
    "met een discount van 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1dcbbc-4e8b-4c92-9bdf-763081bd5a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -0.15  | 5.31   | 17.95  | 0.0    | \n",
      "-------------------------------------\n",
      "| 0.06   | 1.4    | 10.69  | 20.18  | \n",
      "-------------------------------------\n",
      "| 3.71   | 1.24   | 0.07   | 0.87   | \n",
      "-------------------------------------\n",
      "| 0.0    | 3.67   | -1.78  | -3.71  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values2 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_random, discount=0.9)\n",
    "show_utility(random_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8d5cf-27a5-4a1f-b97d-d1c9fe230379",
   "metadata": {},
   "source": [
    "#### MC Optimal policy\n",
    "met discount van 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f5d4ea-4d26-44ee-955f-7b50e0534ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 34.67  | 37.16  | 39.62  | 0.0    | \n",
      "-------------------------------------\n",
      "| 31.95  | 34.03  | 36.26  | 38.12  | \n",
      "-------------------------------------\n",
      "| 27.3   | 31.03  | 28.51  | 27.06  | \n",
      "-------------------------------------\n",
      "| 0.0    | 26.25  | 26.14  | 24.64  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_optimal, discount=1.0)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a295c38-d3da-4b99-977c-274e99f4c4a2",
   "metadata": {},
   "source": [
    "met discount van 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab0bcd9-704d-4885-a0fa-e17cc7197f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 24.15  | 32.26  | 39.54  | 0.0    | \n",
      "-------------------------------------\n",
      "| 18.83  | 24.27  | 29.12  | 36.7   | \n",
      "-------------------------------------\n",
      "| 14.02  | 18.19  | 14.45  | 20.22  | \n",
      "-------------------------------------\n",
      "| 0.0    | 13.05  | 11.1   | 15.09  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_optimal, discount=0.9)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24275c94-340b-4709-bbb4-b42c67d88d15",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning\n",
    "#### functies voor het genereren van één stap met behulp van een policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6822f97-46ed-477d-a47a-7f7c3cdb42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_step(env: Maze, values: np.ndarray, pos: Tuple[int, int], discount: float, p_action: float) -> Tuple[Tuple[int, int], int]:\n",
    "    \"\"\"Picks the next action based on the current state and the random policy.\n",
    "    \n",
    "    args:\n",
    "        env (Maze): The environment which holds the rewards of all the possible states.\n",
    "        values (np.ndarray): The urrent values of the states in the env.\n",
    "        pos (Tuple[int, int]): The current position from which an action should be taken.\n",
    "        discount (float): The discount factor.\n",
    "        p_action (float): The probability of a succesfull action.\n",
    "\n",
    "    returns:\n",
    "        Tuple[Tuple[int, int], int]: A tuple with the action and corresponding reward of the next step.\n",
    "    \n",
    "    \"\"\"\n",
    "    next_actions = env.get_next_action_positions(pos)\n",
    "    \n",
    "    # choose a random action\n",
    "    action = rd.choice(next_actions)\n",
    "    reward = env.R[action]\n",
    "    \n",
    "    # return the current state, the action taken and the reward of the state after the action\n",
    "    return action, reward\n",
    "\n",
    "\n",
    "def get_optimal_step(env: Maze, values: np.ndarray, pos: Tuple[int, int], discount: float, p_action: float) -> Tuple[Tuple[int, int], int]:\n",
    "    \"\"\"Picks the next action based on the current state and the optimal policy.\n",
    "\n",
    "    args:\n",
    "        env (Maze): The environment which holds the rewards of all the possible states.\n",
    "        values (np.ndarray): The urrent values of the states in the env.\n",
    "        pos (Tuple[int, int]): The current position from which an action should be taken.\n",
    "        discount (float): The discount factor.\n",
    "        p_action (float): The probability of a succesfull action.\n",
    "\n",
    "    returns:\n",
    "        Tuple[Tuple[int, int], int]: A tuple with the action and corresponding reward of the next step.\n",
    "    \"\"\"\n",
    "    # get the next action based on the optimal policy\n",
    "    next_actions = env.get_next_action_positions(pos)\n",
    "    action_values = []\n",
    "    \n",
    "    # calculate the value of the next actions based on the values calculated during the value iteration step\n",
    "    for action in next_actions:\n",
    "        action_values.append(env.R[action] + (discount * values[action]))\n",
    "\n",
    "    # get the index of the max elements \n",
    "    max_elem = max(action_values)\n",
    "    policy_actions = [act for i, act in zip(action_values, next_actions) if i == max_elem]\n",
    "    \n",
    "    # choose the desired action and check based on the p_action if the action is certain\n",
    "    action = rd.choice(policy_actions)\n",
    "    if p_action < rd.random():\n",
    "        # whoops, the desired action cannot be taken, so choose one of the others\n",
    "        chosen_index = np.argmax(action_values)\n",
    "        # remove the earlier chosen action and choose a random action\n",
    "        action = rd.choice(next_actions[:chosen_index] + next_actions[chosen_index + 1:])\n",
    "\n",
    "    # gather the reward of the taken action\n",
    "    reward = env.R[action]\n",
    "    \n",
    "    return action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ddaaad0-bda5-4660-82f8-01ae4284ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference_learning(env: Maze, values: np.ndarray, policy: callable, step_size: float = 0.1,\n",
    "                                 discount: float = 0.9, n_episodes: int = 10000, p_action: float = 0.7) -> np.ndarray:\n",
    "    \"\"\"A function that uses temporal difference learning to get a value-function.\n",
    "\n",
    "    args:\n",
    "        env (Maze): The environment which holds the rewards of all the possible states.\n",
    "        values (np.ndarray): The utility values of the states.\n",
    "        policy (callable): The policy that is used to take a step.\n",
    "        step_size (float, optional): The size of the step in a particular direction. Defaults to 0.1.\n",
    "        discount (float, optional): The discount factor. Defaults to 0.9.\n",
    "        n_episodes (int, optional): The amount of episodes to run. Defaults to 10000.\n",
    "        p_action (float, optional): The probability of a succesfull action. Defaults to 0.7.        \n",
    "\n",
    "    returns:\n",
    "        np.ndarray: Returns the calculated values per state.\n",
    "    \"\"\"\n",
    "    state_values = np.zeros(env.R.shape)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # get the random first position\n",
    "        state = env.get_random_position()\n",
    "\n",
    "        while state not in env.end_states:\n",
    "            \n",
    "            # choose an action based on the policy\n",
    "            action, reward = policy(env, values, state, discount, p_action)\n",
    "            \n",
    "            # update the value of the current_state\n",
    "            state_values[state] = state_values[state] + step_size * (reward + discount * state_values[action] - state_values[state])\n",
    "            \n",
    "            # update the current state\n",
    "            state = action\n",
    "\n",
    "    return state_values   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517497cd-54f9-4c55-b278-bbdf1d7beb68",
   "metadata": {},
   "source": [
    "#### TD Random policy\n",
    "met discount van 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c534a7-ae4a-4542-9022-c06b5e1699df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -12.29 | -7.47  | 12.7   | 0.0    | \n",
      "-------------------------------------\n",
      "| -8.98  | -12.05 | -4.19  | 1.81   | \n",
      "-------------------------------------\n",
      "| 0.08   | -9.95  | -15.47 | -16.26 | \n",
      "-------------------------------------\n",
      "| 0.0    | -6.55  | -15.36 | -16.77 | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values1 = temporal_difference_learning(maze, values, policy=get_random_step, discount=1.0, p_action=0.7)\n",
    "show_utility(random_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e248dab-8a14-46a5-bec5-f8690104c60e",
   "metadata": {},
   "source": [
    "met discount van 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f170209-dc80-4ae8-8a56-c572b0baba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -5.24  | -4.01  | 5.58   | 0.0    | \n",
      "-------------------------------------\n",
      "| -4.26  | -6.12  | -4.13  | 2.63   | \n",
      "-------------------------------------\n",
      "| 0.94   | -5.43  | -7.98  | -9.52  | \n",
      "-------------------------------------\n",
      "| 0.0    | 0.07   | -6.96  | -8.29  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values2 = temporal_difference_learning(maze, values, policy=get_random_step, discount=0.9, p_action=0.7)\n",
    "show_utility(random_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fe73a-dae8-4366-a265-89216b3a4325",
   "metadata": {},
   "source": [
    "#### TD Optimal policy\n",
    "met discount van 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "741e77aa-2461-4145-b5c8-6e291b61545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 32.43  | 35.68  | 36.91  | 0.0    | \n",
      "-------------------------------------\n",
      "| 30.78  | 33.3   | 32.16  | 33.67  | \n",
      "-------------------------------------\n",
      "| 26.33  | 29.91  | 27.49  | 24.72  | \n",
      "-------------------------------------\n",
      "| 0.0    | 27.91  | 25.26  | 23.92  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = temporal_difference_learning(maze, values, policy=get_optimal_step, discount=1.0, p_action=0.7)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04a9161e-c94a-4ec4-abf6-d2ba71204ba3",
   "metadata": {},
   "source": [
    "met discount van 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "788e0adf-9a98-4a46-a86d-c35de1600159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 21.64  | 27.9   | 35.24  | 0.0    | \n",
      "-------------------------------------\n",
      "| 15.78  | 20.36  | 25.46  | 32.71  | \n",
      "-------------------------------------\n",
      "| 12.41  | 14.79  | 13.21  | 18.11  | \n",
      "-------------------------------------\n",
      "| 0.0    | 11.21  | 9.75   | 13.01  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values2 = temporal_difference_learning(maze, values, policy=get_optimal_step, discount=0.9, p_action=0.7)\n",
    "show_utility(optim_values2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
