{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a957b574-21d2-418f-84c7-22532d721458",
   "metadata": {},
   "source": [
    "# Inleveropgave 2: Model-Free Prediction and Control\n",
    "\n",
    "## Model-Free Prediction\n",
    "\n",
    "### Sources\n",
    "- https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3598de3e-82f0-4eaf-bf8b-91339b517f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from collections import defaultdict\n",
    "from utils import Maze, show_utility, value_iteration\n",
    "\n",
    "import random as rd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb13a94-7c2f-4e6e-a870-b7ba6f5017dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d43907e9-1ddd-4a20-adab-ce08854c1e9d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[30.5   , 35.    , 40.    ,  0.    ],\n",
       "       [26.45  , 30.5   , 35.    , 40.    ],\n",
       "       [22.805 , 26.45  , 22.805 , 26.    ],\n",
       "       [ 0.    , 22.805 , 19.5245, 22.4   ]])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "start_state = (3, 2)\n",
    "terminal_states = [(0, 3), (3, 0)]\n",
    "\n",
    "rewards = np.array([[-1, -1, -1, 40],\n",
    "                    [-1, -1, -10, -10],\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [10, -2, -1, -1]])\n",
    "\n",
    "# initialize the Maze\n",
    "maze = Maze(rewards, terminal_states, start_state)\n",
    "\n",
    "# use the value function to get the utilities\n",
    "values = value_iteration(maze, discount=0.9, p_action=1.0)\n",
    "values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb4fd2-06be-4ad4-83f1-e1ef5a7d3009",
   "metadata": {},
   "source": [
    "### Generating episodes\n",
    "Voor het genereren van een episode worden er twee nieuwe functies ge√Øntroduceerd. De eerste functie kan gebruikt worden om een random episode te genereren, terwijl de tweede gebruikt maakt van de eerder uitgewerkte value iteration om zodanig een bepaalde policy te kunnen volgen.\n",
    "\n",
    "Deze functies hebben dezelfde hoeveelheid parameters. Echter, niet al deze parameters worden gebruikt (dit zorgt voor iets meer consistency)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cfb4a389-b682-49e7-b53d-969057e25b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_random(env: Maze, values: np.ndarray, discount: float, p_action: float):\n",
    "    \"\"\"Generates an episode based on the random policy.\n",
    "    \n",
    "    Here the p_action is not being used, because the actions are already random and therefore it won't really matter if we take a wrong turn\n",
    "    Furthermore the discount is not being used.\n",
    "    Both these parameters are set in the function because API consistency\n",
    "    \"\"\"\n",
    "    steps = []  # holds Tuples with the states, actions and rewards\n",
    "    pos = env.get_random_position()\n",
    "    \n",
    "    # break if the chosen state is a terminal state\n",
    "    while pos not in env.end_states:\n",
    "\n",
    "        next_actions = env.get_next_action_positions(pos)\n",
    "        # choose a random action and get the reward for the action.\n",
    "        action = rd.choice(next_actions)\n",
    "        \n",
    "        reward = env.R[action]\n",
    "        steps.append((pos, action, reward))\n",
    "        # update the pos to the taken action\n",
    "        pos = action\n",
    "        \n",
    "    # save the latest pos with all extra data\n",
    "    steps.append((pos, (), 0))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3b5665c6-82de-4a1c-bcc0-4bcf20f5ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_optimal(env: Maze, values: np.ndarray, discount: float, p_action: float):\n",
    "    \"\"\"Generates an episode based on the optimal policy.\"\"\"\n",
    "    steps = []  # holds Tuples with the states, actions and rewards\n",
    "    pos = env.get_random_position()\n",
    "    \n",
    "    # break if the chosen state is a terminal state\n",
    "    while pos not in env.end_states:\n",
    "    \n",
    "        # get the next action based on the optimal policy\n",
    "        next_actions = env.get_next_action_positions(pos)\n",
    "        action_values = []\n",
    "        \n",
    "        for action in next_actions:\n",
    "            action_values.append(env.R[action] + (discount * values[action]))\n",
    "        \n",
    "        # get the index of the max elements\n",
    "        max_elem = max(action_values)\n",
    "        policy_actions = [act for i, act in zip(action_values, next_actions) if i == max_elem]\n",
    "        \n",
    "        # choose the desired action and check based on the p_action if the action is certain\n",
    "        action = rd.choice(policy_actions)\n",
    "        if p_action < rd.random():\n",
    "            # whoops, the desired action cannot be taken, so choose one of the others\n",
    "            chosen_index = np.argmax(action_values)\n",
    "            # remove the earlier chosen action and choose a random action\n",
    "            action = rd.choice(next_actions[:chosen_index] + next_actions[chosen_index + 1:])\n",
    "        \n",
    "        reward = env.R[action]\n",
    "        steps.append((pos, action, reward))\n",
    "        # update the pos to the taken action\n",
    "        pos = action\n",
    "        \n",
    "    # save the latest pos with all extra data\n",
    "    steps.append((pos, (), 0))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db010d7-3439-434b-9ef9-fceaa9932385",
   "metadata": {},
   "source": [
    "### Monte-Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b1249c51-3b13-4b32-89b2-62ebcecf3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_evaluation(env: Maze, values: np.ndarray, policy: callable, discount: float = 0.9,\n",
    "                              n_episodes: int = 10000, p_action: float = 0.7):\n",
    "    \"\"\"\"\"\"\n",
    "    state_values = np.zeros(env.R.shape)\n",
    "    state_returns = defaultdict(list)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # generate a new episode with a certain policy\n",
    "        episode = policy(env, values, discount, p_action)\n",
    "\n",
    "        G = 0\n",
    "        visited_states = []\n",
    "        # looping over each step and \n",
    "        for pos, action, reward in episode[::-1]:\n",
    "            G = discount * G + reward\n",
    "            \n",
    "            if pos not in visited_states:\n",
    "                # update the the current state with the new return\n",
    "                state_returns[pos].append(G)\n",
    "                # calculate the average value\n",
    "                state_values[pos] = np.mean(state_returns[pos])\n",
    "                # update visited states\n",
    "                visited_states.append(pos)\n",
    "    \n",
    "    return state_values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f336f-4141-4463-8a1a-371efe97f776",
   "metadata": {},
   "source": [
    "#### MC Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f32ccf42-5f32-45dc-94cf-a85b09b32ce9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 0.08   | 5.63   | 16.84  | 0.0    | \n",
      "-------------------------------------\n",
      "| 1.78   | 5.35   | 15.25  | 19.95  | \n",
      "-------------------------------------\n",
      "| 3.64   | 3.09   | 3.83   | 2.16   | \n",
      "-------------------------------------\n",
      "| 0.0    | 2.83   | -0.74  | -4.19  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_random, discount=1.0)\n",
    "show_utility(random_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "db1dcbbc-4e8b-4c92-9bdf-763081bd5a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -0.4   | 4.83   | 17.22  | 0.0    | \n",
      "-------------------------------------\n",
      "| 0.11   | 1.18   | 10.37  | 19.96  | \n",
      "-------------------------------------\n",
      "| 3.57   | 0.8    | -0.6   | 0.24   | \n",
      "-------------------------------------\n",
      "| 0.0    | 3.33   | -2.1   | -3.93  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values2 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_random, discount=0.9)\n",
    "show_utility(random_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8d5cf-27a5-4a1f-b97d-d1c9fe230379",
   "metadata": {},
   "source": [
    "#### MC Optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b5f5d4ea-4d26-44ee-955f-7b50e0534ed5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 34.45  | 37.12  | 39.54  | 0.0    | \n",
      "-------------------------------------\n",
      "| 31.59  | 34.16  | 36.36  | 37.93  | \n",
      "-------------------------------------\n",
      "| 27.69  | 31.49  | 28.85  | 26.69  | \n",
      "-------------------------------------\n",
      "| 0.0    | 27.13  | 26.24  | 24.76  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_optimal, discount=1.0)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6ab0bcd9-704d-4885-a0fa-e17cc7197f8a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 24.56  | 32.18  | 39.52  | 0.0    | \n",
      "-------------------------------------\n",
      "| 18.98  | 24.25  | 29.25  | 36.56  | \n",
      "-------------------------------------\n",
      "| 14.21  | 18.29  | 14.27  | 19.76  | \n",
      "-------------------------------------\n",
      "| 0.0    | 12.84  | 10.62  | 14.18  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = monte_carlo_policy_evaluation(maze, values, policy=generate_episode_optimal, discount=0.9)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24275c94-340b-4709-bbb4-b42c67d88d15",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a6822f97-46ed-477d-a47a-7f7c3cdb42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_random_step(env: Maze, values: np.ndarray, pos: Tuple[int, int], discount: float, p_action: float):\n",
    "    \"\"\"Picks the next action based on the current state and the random policy.\"\"\"\n",
    "    next_actions = env.get_next_action_positions(pos)\n",
    "    \n",
    "    # choose a random action\n",
    "    action = rd.choice(next_actions)\n",
    "    reward = env.R[action]\n",
    "    \n",
    "    # return the current state, the action taken and the reward of the state after the action\n",
    "    return pos, action, reward\n",
    "\n",
    "\n",
    "def get_optimal_step(env: Maze, values: np.ndarray, pos: Tuple[int, int], discount: float, p_action: float):\n",
    "    \"\"\"Picks the next action based on the current state and the optimal policy.\"\"\"\n",
    "    \n",
    "    # get the next action based on the optimal policy\n",
    "    next_actions = env.get_next_action_positions(pos)\n",
    "    action_values = []\n",
    "    \n",
    "    # calculate the value of the next actions based on the values calculated during the value iteration step\n",
    "    for action in next_actions:\n",
    "        action_values.append(env.R[action] + (discount * values[action]))\n",
    "\n",
    "    # get the index of the max elements \n",
    "    max_elem = max(action_values)\n",
    "    policy_actions = [act for i, act in zip(action_values, next_actions) if i == max_elem]\n",
    "    \n",
    "    # choose the desired action and check based on the p_action if the action is certain\n",
    "    action = rd.choice(policy_actions)\n",
    "    if p_action < rd.random():\n",
    "        # whoops, the desired action cannot be taken, so choose one of the others\n",
    "        chosen_index = np.argmax(action_values)\n",
    "        # remove the earlier chosen action and choose a random action\n",
    "        action = rd.choice(next_actions[:chosen_index] + next_actions[chosen_index + 1:])\n",
    "\n",
    "    # gather the reward of the taken action\n",
    "    reward = env.R[action]\n",
    "    \n",
    "    return pos, action, reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0ddaaad0-bda5-4660-82f8-01ae4284ea61",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference_learning(env: Maze, values: np.ndarray, policy: callable, step_size: float = 0.1,\n",
    "                                 discount: float = 0.9, n_episodes: int = 10000, p_action: float = 0.7):\n",
    "    \"\"\"\"\"\"\n",
    "    state_values = np.zeros(env.R.shape)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # get the random first position\n",
    "        state = env.get_random_position()\n",
    "\n",
    "        while state not in env.end_states:\n",
    "            \n",
    "            # choose an action based on the policy\n",
    "            state, action, reward = policy(env, values, state, discount, p_action)\n",
    "            \n",
    "            # update the value of the current_state\n",
    "            state_values[state] = state_values[state] + step_size * (reward + discount * state_values[action] - state_values[state])\n",
    "            \n",
    "            # update the current state\n",
    "            state = action\n",
    "\n",
    "    return state_values   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "517497cd-54f9-4c55-b278-bbdf1d7beb68",
   "metadata": {},
   "source": [
    "#### TD Random policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "03c534a7-ae4a-4542-9022-c06b5e1699df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -14.3  | -9.42  | 2.55   | 0.0    | \n",
      "-------------------------------------\n",
      "| -13.17 | -16.5  | -15.0  | -12.26 | \n",
      "-------------------------------------\n",
      "| -5.68  | -12.05 | -19.58 | -22.13 | \n",
      "-------------------------------------\n",
      "| 0.0    | -5.49  | -16.37 | -21.3  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values1 = temporal_difference_learning(maze, values, policy=get_random_step, discount=1.0, p_action=0.7)\n",
    "show_utility(random_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f170209-dc80-4ae8-8a56-c572b0baba94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -5.9   | -5.26  | 9.49   | 0.0    | \n",
      "-------------------------------------\n",
      "| -5.29  | -10.28 | -6.25  | -4.22  | \n",
      "-------------------------------------\n",
      "| -2.07  | -7.98  | -10.52 | -11.14 | \n",
      "-------------------------------------\n",
      "| 0.0    | -3.36  | -8.2   | -9.58  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values2 = temporal_difference_learning(maze, values, policy=get_random_step, discount=0.9, p_action=0.7)\n",
    "show_utility(random_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "113fe73a-dae8-4366-a265-89216b3a4325",
   "metadata": {},
   "source": [
    "#### TD Optimal policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "741e77aa-2461-4145-b5c8-6e291b61545a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 33.81  | 34.59  | 34.38  | 0.0    | \n",
      "-------------------------------------\n",
      "| 29.91  | 31.19  | 32.73  | 29.18  | \n",
      "-------------------------------------\n",
      "| 26.83  | 29.13  | 25.95  | 22.56  | \n",
      "-------------------------------------\n",
      "| 0.0    | 25.21  | 25.11  | 24.19  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = temporal_difference_learning(maze, values, policy=get_optimal_step, discount=1.0, p_action=0.7)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "788e0adf-9a98-4a46-a86d-c35de1600159",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 23.23  | 26.64  | 34.47  | 0.0    | \n",
      "-------------------------------------\n",
      "| 15.81  | 18.11  | 26.54  | 32.73  | \n",
      "-------------------------------------\n",
      "| 12.13  | 12.42  | 11.51  | 18.41  | \n",
      "-------------------------------------\n",
      "| 0.0    | 10.28  | 9.65   | 15.24  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values2 = temporal_difference_learning(maze, values, policy=get_optimal_step, discount=0.9, p_action=0.7)\n",
    "show_utility(optim_values2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
