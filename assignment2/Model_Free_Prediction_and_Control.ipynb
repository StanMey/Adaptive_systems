{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a957b574-21d2-418f-84c7-22532d721458",
   "metadata": {},
   "source": [
    "# Inleveropgave 2: Model-Free Prediction and Control\n",
    "\n",
    "### Sources\n",
    "- https://towardsdatascience.com/reinforcement-learning-rl-101-with-python-e1aa0d37d43b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3598de3e-82f0-4eaf-bf8b-91339b517f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Tuple, List\n",
    "from collections import defaultdict\n",
    "from utils import show_utility\n",
    "\n",
    "import random as rd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb13a94-7c2f-4e6e-a870-b7ba6f5017dc",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22c9c5b6-2195-4fe1-a38a-1715f18454fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Maze:\n",
    "    def __init__(self, R: np.ndarray, end: List[Tuple[int, int]], start: Tuple[int, int]):\n",
    "        \"\"\"The init function.\n",
    "        \n",
    "        args:\n",
    "            R (np.ndarray): The array with the rewards per state.\n",
    "            end (List[Tuple[int, int]]): A list with the positions of the end states.\n",
    "        \"\"\"\n",
    "        self.R = R\n",
    "        self.end_states = end\n",
    "        self.start_state = start\n",
    "        self.states = np.arange(self.R.size).reshape(self.R.shape)\n",
    "    \n",
    "    def get_next_action_positions(self, pos: Tuple[int, int]):\n",
    "        \"\"\"asdf.\n",
    "        \n",
    "        args:\n",
    "            pos (Tuple[int, int]): .\n",
    "        \"\"\"\n",
    "        row, col = pos\n",
    "\n",
    "        up = (row - 1, col) if row - 1 >= 0 else pos\n",
    "        right = (row, col + 1) if col + 1 < self.R.shape[1] else pos\n",
    "        left = (row, col -1) if col - 1 >= 0 else pos\n",
    "        down = (row + 1, col) if row + 1 < self.R.shape[0] else pos\n",
    "        return up, right, left, down\n",
    "    \n",
    "    def get_random_position(self):\n",
    "        \"\"\"Returns a random position in the environment.\"\"\"\n",
    "        return (rd.randint(0, self.R.shape[1] - 1), rd.randint(0, self.R.shape[0] - 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "657b3db1-9533-4127-9fc7-6ef27aa87a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def value_iteration(env: Maze, end_states: List[Tuple[int, int]], p_action: float = 0.7, discount: float = 0.9, threshold: float = 0.0001):\n",
    "    \n",
    "    utility = np.zeros(env.R.shape)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     def __calc_action_value(self, action, other_actions) -> float:\n",
    "#         \"\"\"Calculates the action value of a certain action with the succesfullness probability of the actions.\"\"\"\n",
    "#         total_value = self.p_action * (self.R[action] + (self.discount * self.utility[action]))\n",
    "        \n",
    "#         dist_chance = (1 - self.p_action) / len(other_actions)\n",
    "        \n",
    "#         for noise_action in other_actions:\n",
    "#             total_value += dist_chance * (self.R[noise_action] + (self.discount * self.utility[noise_action]))\n",
    "#         return total_value\n",
    "    \n",
    "#     def get_greedy_policy(self, current_pos: Tuple[int, int]) -> List[int]:\n",
    "#         \"\"\"Gets the best policy based on greediness for each state.\n",
    "        \n",
    "#         args:\n",
    "#             current_pos (Tuple[int, int]): The state for which the policy should be calculated.\n",
    "\n",
    "#         returns:\n",
    "#             List[int]: The actions to take based on the calculated policy.\n",
    "#         \"\"\"\n",
    "#         actions = self.__get_action_positions(current_pos)\n",
    "#         action_values = []\n",
    "#         for index, action in enumerate(actions):\n",
    "#             noise_actions = actions[:index] + actions[index+1:]\n",
    "#             action_values.append(self.__calc_action_value(action, noise_actions))\n",
    "\n",
    "#         # get the index of the max elements \n",
    "#         max_elem = max(action_values)\n",
    "#         return [1 if i == max_elem else 0 for i in action_values]\n",
    "    \n",
    "#     def value_iteration(self):\n",
    "#         \"\"\"Value iteration method.\"\"\"\n",
    "#         delta = np.inf\n",
    "#         # get all positions in the grid\n",
    "#         positions = [(i,j) for i in range(self.R.shape[0]) for j in range(self.R.shape[1])]\n",
    "        \n",
    "#         while delta > self.threshold:\n",
    "#             delta = 0\n",
    "#             new_utility = np.zeros(self.utility.shape)\n",
    "#             for pos in positions:\n",
    "#                 # check if we are evaluating an end state\n",
    "#                 if pos in self.end_states:\n",
    "#                     # current position is an end-state so value is 0\n",
    "#                     continue\n",
    "\n",
    "#                 # save the current value\n",
    "#                 value = self.utility[pos]\n",
    "#                 # get the next positions of all the actions that can be taken on the current positions\n",
    "#                 actions = self.__get_action_positions(pos)\n",
    "#                 action_values = []\n",
    "#                 for index, action in enumerate(actions):\n",
    "#                     noise_actions = actions[:index] + actions[index+1:]\n",
    "#                     action_values.append(self.__calc_action_value(action, noise_actions))\n",
    "                \n",
    "#                 # select the action with the highest utility\n",
    "#                 highest_utility = max(action_values)\n",
    "#                 new_utility[pos] = highest_utility\n",
    "#                 # update the delta\n",
    "#                 delta = max(delta, abs(value - highest_utility))\n",
    "            \n",
    "#             self.utility = copy.deepcopy(new_utility)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d43907e9-1ddd-4a20-adab-ce08854c1e9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_state = (3, 2)\n",
    "terminal_states = [(0, 3), (3, 0)]\n",
    "\n",
    "rewards = np.array([[-1, -1, -1, 40],\n",
    "                    [-1, -1, -10, -10],\n",
    "                    [-1, -1, -1, -1],\n",
    "                    [10, -2, -1, -1]])\n",
    "\n",
    "# initialize the Maze\n",
    "maze = Maze(rewards, terminal_states, start_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17bb4fd2-06be-4ad4-83f1-e1ef5a7d3009",
   "metadata": {},
   "source": [
    "## Model-Free Prediction\n",
    "### Monte-Carlo Policy Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfb4a389-b682-49e7-b53d-969057e25b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_random(env: Maze):\n",
    "    \"\"\"\"\"\"\n",
    "    steps = []  # holds Tuples with the states, actions and rewards\n",
    "    pos = env.get_random_position()\n",
    "    \n",
    "    # break if the chosen state is a terminal state\n",
    "    while pos not in env.end_states:\n",
    "\n",
    "        next_actions = env.get_next_action_positions(pos)\n",
    "        # choose a random action and gather\n",
    "        action = rd.choice(next_actions)\n",
    "        \n",
    "        reward = env.R[action]\n",
    "        steps.append((pos, action, reward))\n",
    "        # update the pos to the taken action\n",
    "        pos = action\n",
    "        \n",
    "    # save the latest pos with all extra data\n",
    "    steps.append((pos, (), 0))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3b5665c6-82de-4a1c-bcc0-4bcf20f5ab42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_episode_optimal(env: Maze):\n",
    "    \"\"\"\"\"\"\n",
    "    steps = []  # holds Tuples with the states, actions and rewards\n",
    "    pos = env.get_random_position()\n",
    "    \n",
    "    # break if the chosen state is a terminal state\n",
    "    while pos not in env.end_states:\n",
    "\n",
    "        next_actions = env.get_next_action_positions(pos)\n",
    "        # choose a random action and gather\n",
    "        action = rd.choice(next_actions)\n",
    "        \n",
    "        reward = env.R[action]\n",
    "        steps.append((pos, action, reward))\n",
    "        # update the pos to the taken action\n",
    "        pos = action\n",
    "        \n",
    "    # save the latest pos with all extra data\n",
    "    steps.append((pos, (), 0))\n",
    "\n",
    "    return steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "b1249c51-3b13-4b32-89b2-62ebcecf3213",
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_evaluation(env: Maze, policy: str = \"random\",\n",
    "                              discount: float = 0.9, n_episodes: int = 1000):\n",
    "    \"\"\"\"\"\"\n",
    "    values = np.zeros(env.R.shape)\n",
    "    state_returns = defaultdict(list)\n",
    "\n",
    "    for _ in range(n_episodes):\n",
    "        # generate a new episode with a certain policy\n",
    "        if policy == \"random\":\n",
    "            # random policy\n",
    "            episode = generate_episode_random(env)\n",
    "        elif policy == \"optimal\":\n",
    "            # optimal policy\n",
    "            episode = [((), (), 10)]\n",
    "\n",
    "        G = 0\n",
    "        visited_states = []\n",
    "        # looping over each step and \n",
    "        for pos, action, reward in episode[::-1]:\n",
    "            G = discount * G + reward\n",
    "            \n",
    "            if pos not in visited_states:\n",
    "                # update the the current state with the new return\n",
    "                state_returns[pos].append(G)\n",
    "                # calculate the average value\n",
    "                values[pos] = np.mean(state_returns[pos])\n",
    "                # update visited states\n",
    "                visited_states.append(pos)\n",
    "    \n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf4f336f-4141-4463-8a1a-371efe97f776",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "f32ccf42-5f32-45dc-94cf-a85b09b32ce9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 0.42   | 5.25   | 15.02  | 0.0    | \n",
      "-------------------------------------\n",
      "| 3.8    | 6.17   | 14.03  | 19.18  | \n",
      "-------------------------------------\n",
      "| 4.65   | 5.37   | 4.57   | 3.8    | \n",
      "-------------------------------------\n",
      "| 0.0    | 3.84   | -1.05  | -4.3   | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values1 = monte_carlo_policy_evaluation(maze, policy=\"random\", discount=1.0)\n",
    "show_utility(random_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "db1dcbbc-4e8b-4c92-9bdf-763081bd5a9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 0.12   | 5.78   | 17.88  | 0.0    | \n",
      "-------------------------------------\n",
      "| 0.01   | 0.73   | 9.41   | 19.32  | \n",
      "-------------------------------------\n",
      "| 3.56   | 0.95   | -0.88  | -0.05  | \n",
      "-------------------------------------\n",
      "| 0.0    | 4.2    | -1.08  | -3.45  | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "random_values2 = monte_carlo_policy_evaluation(maze, policy=\"random\", discount=0.9)\n",
    "show_utility(random_values2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b8d5cf-27a5-4a1f-b97d-d1c9fe230379",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b5f5d4ea-4d26-44ee-955f-7b50e0534ed5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| -720.0 | 1486.0 | 6015.0 | 0.0    | \n",
      "-------------------------------------\n",
      "| -373.0 | 1971.0 | 7535.0 | 7823.0 | \n",
      "-------------------------------------\n",
      "| 1392.0 | 1187.0 | 1782.0 | 837.0  | \n",
      "-------------------------------------\n",
      "| 0.0    | 1132.0 | -789.0 | -1905.0 | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = monte_carlo_policy_evaluation(maze, policy=\"optimal\", discount=1.0)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6ab0bcd9-704d-4885-a0fa-e17cc7197f8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------------------------\n",
      "| 10000.0 | 10000.0 | 10000.0 | 10000.0 | \n",
      "-------------------------------------\n",
      "| 10000.0 | 10000.0 | 10000.0 | 10000.0 | \n",
      "-------------------------------------\n",
      "| 10000.0 | 10000.0 | 10000.0 | 10000.0 | \n",
      "-------------------------------------\n",
      "| 10000.0 | 10000.0 | 10000.0 | 10000.0 | \n",
      "-------------------------------------\n"
     ]
    }
   ],
   "source": [
    "optim_values1 = monte_carlo_policy_evaluation(maze, policy=\"optimal\", discount=0.9)\n",
    "show_utility(optim_values1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24275c94-340b-4709-bbb4-b42c67d88d15",
   "metadata": {},
   "source": [
    "### Temporal Difference Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a6822f97-46ed-477d-a47a-7f7c3cdb42af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def temporal_difference_learning(env: Maze, policy: str = \"optimal\",\n",
    "                                 discount: float = 0.9, n_episodes: int = 100):\n",
    "    \"\"\"\"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03c534a7-ae4a-4542-9022-c06b5e1699df",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f170209-dc80-4ae8-8a56-c572b0baba94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed8d1a0a-ae37-449f-919e-5ace40f36b34",
   "metadata": {},
   "source": [
    "## Model-Free Control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "331d84e1-08b2-4cbc-b125-b9be498f113f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b119b1b1-ba9f-4dfd-8a76-65c4df7b4f7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
